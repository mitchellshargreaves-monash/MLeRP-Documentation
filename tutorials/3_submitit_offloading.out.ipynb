{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dask Offloading\n",
        "\n",
        "SubmitIt is a lower level library than Dask which you can also use to\n",
        "offload parts of your notebook to the SLURM queue. Rather than managing\n",
        "a cluster, you will instead directly be submitting python functions to\n",
        "the SLURM queue giving you more control. For more information, have a\n",
        "read of their [PyPi page](https://pypi.org/project/submitit/)."
      ],
      "id": "9bdc6196-4aa1-4496-b4b1-166d4afe5f07"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import submitit\n",
        "\n",
        "# Define where we'd like submitit to place our logs\n",
        "executor = submitit.AutoExecutor(folder='~/submitit_logs')\n",
        "\n",
        "# Define the parameters of our slurm job\n",
        "# Just like Dasks' job_extra_directives, additional_parameters allows us to specify things that submitit doesn't support directly\n",
        "executor.update_parameters(timeout_min=30, mem_gb=128, cpus_per_task=16, slurm_partition=\"BigCats\", additional_parameters={\"gres\": \"gpu:1\"})"
      ],
      "id": "13e668fc-ee7b-4535-840f-d06859c23c36"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can submit our function to the cluster with the `executor.submit`\n",
        "method. This will return a future which can be unpacked with its result\n",
        "using `future.result()` just like when we were working with Dask.\n",
        "Because we are offloading to the SLURM queue print statements will not\n",
        "be visible, just like with Dask `SLURMClusters`. However, the full stack\n",
        "trace is still visible when an error or assertion is raised within the\n",
        "function."
      ],
      "id": "7ddcf5f3-8ff2-4fa6-9754-51e750caf7b5"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def client_test(input1, input2, error=False, test=False):\n",
        "    # Force an error\n",
        "    if error:\n",
        "        assert 0 == 1\n",
        "    \n",
        "    # Stop after one batch when testing        \n",
        "    if test: \n",
        "        print(\"When running in a local cluster you can see print statements!\")\n",
        "\n",
        "    return input1, input2"
      ],
      "id": "22f0cdca-927a-47c8-b05e-20208a6f2582"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "('input1', 'input2')"
            ]
          }
        }
      ],
      "source": [
        "future = executor.submit(client_test, \"input1\", \"input2\", test=True)\n",
        "future.result()"
      ],
      "id": "f7ebacb5-561b-4544-ac8f-967dd7f3a6b8"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "future = client.submit(client_test, \"input1\", \"input2\", error=True)\n",
        "future.result()"
      ],
      "id": "cdf67062-f97b-472f-827e-681cae574b5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that since we are interacting directly with the queue, we don’t\n",
        "need to clean up and shut down our cluster when using SubmitIt.\n",
        "\n",
        "If needed we can be more specific about the specific GPU type and QoS we\n",
        "need if we have more complex requirements."
      ],
      "id": "fcac7cf1-4dcf-42f7-a0dc-29f15a1dc51d"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "('input1', 'input2')"
            ]
          }
        }
      ],
      "source": [
        "executor.update_parameters(timeout_min=30, mem_gb=128, cpus_per_task=16, slurm_partition=\"BigCats\", additional_parameters={\"gres\": \"gpu:3g.20gb:1\", \"partition\": \"lion\"})\n",
        "client.submit(client_test, \"input1\", \"input2\", test=True).result()"
      ],
      "id": "9f3db53e-0adf-4b43-8d28-83b013eae5f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison with Dask\n",
        "\n",
        "As you can see, we’ve implemented the same use case with both Dask and\n",
        "SubmitIt. Which begs the question - which should you use for your\n",
        "research?\n",
        "\n",
        "Both packages have pros and cons, but on the whole, Dask is much better\n",
        "suited towards tasks which can benefit from being broken into many small\n",
        "tasks - like when preprocessing your data. SubmitIt on the other hand is\n",
        "much better suited for use cases where you are looking to offload one\n",
        "larger job at a time, like when you are training.\n",
        "\n",
        "Of the two, Dask is the more mature package with more flexibility and\n",
        "complete documentation - but if you are looking for a simple offloading\n",
        "package it is often far more complexity than you need."
      ],
      "id": "c66d17c2-9a84-4fe3-b7b5-27bc7b26d2e6"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "db096b8404a4f1f3e1df0cc89f001e138448327417ef835d10f5a76aa612f160"
      }
    }
  }
}