{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dask Offloading\n",
        "\n",
        "### Testing with a `LocalCluster`\n",
        "\n",
        "Once you’ve written your function and are ready to move things over to .\n",
        "Working with Dask workers introduces another layer of complexity where\n",
        "things can go wrong, which make Dask LocalClusters the easiest way to\n",
        "prepare your code for offloading. This will mean that code will execute\n",
        "in the notebook session, just like running your function straight,\n",
        "allowing you to view print statements and debug errors normally rather\n",
        "than dealing with remote code execusion before we’re ready. Once you’re\n",
        "satisfied with your code you can switch over to a SLURMCluster to\n",
        "accelerate with GPU."
      ],
      "id": "7ff8533e-0efb-48c1-8961-490aadf4c7f6"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from distributed import Client, LocalCluster\n",
        "\n",
        "cluster = LocalCluster()\n",
        "client = Client(cluster)"
      ],
      "id": "66f168f3-74ec-4d42-9e89-28359c397f0d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can submit our function to the cluster with the `client.submit`\n",
        "method. This will return a future which can be unpacked with its result\n",
        "using `future.result()`. We can see the outputs of print statements\n",
        "while we’re using a `LocalCluster`. Print statements will not be visible\n",
        "when executing remotely with `SLURMCluster`. Similarly, the full stack\n",
        "trace is still visible when an error or assertion is raised within the\n",
        "function."
      ],
      "id": "a932f715-9d48-4f97-b67d-f8c147319a29"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def client_test(input1, input2, error=False, test=False):\n",
        "    # Force an error\n",
        "    if error:\n",
        "        assert 0 == 1\n",
        "    \n",
        "    # Stop after one batch when testing        \n",
        "    if test: \n",
        "        print(\"When running in a local cluster you can see print statements!\")\n",
        "\n",
        "    return input1, input2"
      ],
      "id": "629ae82f-dfb9-4444-830f-0cf02daa02b7"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When running in a local cluster you can see print statements!"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "('input1', 'input2')"
            ]
          }
        }
      ],
      "source": [
        "future = client.submit(client_test, \"input1\", \"input2\", test=True)\n",
        "future.result()"
      ],
      "id": "bb636c3c-4aeb-4ea2-b036-a83a4ee5e95c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-31 06:32:22,017 - distributed.worker - WARNING - Compute Failed\n",
            "Key:       client_test-821dd3f7995546bf5d9280be38a9afd3\n",
            "Function:  client_test\n",
            "args:      ('input1', 'input2')\n",
            "kwargs:    {'error': True}\n",
            "Exception: 'AssertionError()'\n"
          ]
        }
      ],
      "source": [
        "future = client.submit(client_test, \"input1\", \"input2\", error=True)\n",
        "future.result()"
      ],
      "id": "3a18962c-8e98-4eaf-96ee-7ee2f2b70e2f"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.shutdown()"
      ],
      "id": "3bda2290-fa7d-4209-9432-c3f15ad78bee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running on a `SLURMCluster`\n",
        "\n",
        "We can pass in extra SLURM requirements in job_extra_directives to\n",
        "request a GPU for our jobs. To read more about configuring the\n",
        "`SLURMCluster` to interact with the SLURM queue, go to Dask’s [jobqueue\n",
        "documentation](https://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SLURMCluster.html?highlight=slurmcluster)."
      ],
      "id": "4bf10a51-207d-40c9-a46b-2841ec8ffba9"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dask_jobqueue import SLURMCluster\n",
        "from distributed import Client\n",
        "cluster = SLURMCluster(\n",
        "    memory=\"128g\", processes=1, cores=16, job_extra_directives=[\"--gres=gpu:1\", \"--partition=BigCats\"]\n",
        ")\n",
        "\n",
        "cluster.scale(1)\n",
        "client = Client(cluster)"
      ],
      "id": "344fbb70-b615-4e8c-b0db-99841c695bfc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this code is executing remotely we won’t see our print statements"
      ],
      "id": "219fbc62-d754-4917-a4fd-45cc19bee535"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "('input1', 'input2')"
            ]
          }
        }
      ],
      "source": [
        "client.submit(client_test, \"input1\", \"input2\", test=True).result()"
      ],
      "id": "db7bf652-c92a-4d88-8699-cda93768a1e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dask will raise any errors that the process triggers locally, even when\n",
        "executing remotely - but you may not get the full stack trace"
      ],
      "id": "4ec1cacc-fc54-4272-85b7-87139c95ac3a"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.submit(client_test, \"input1\", \"input2\", error=True).result()"
      ],
      "id": "39281eab-d0a4-449f-b9a1-0074e291ebfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you’re working with any objects that are particularly memory\n",
        "intensive, you can consider using the `client.scatter` method to scatter\n",
        "large objects out to our workers ahead of time for more efficient\n",
        "execution."
      ],
      "id": "8a8487ad-876e-4306-b738-240f13b322cc"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(\"Let's pretend that this string is actually a really big object like your dataset\",\n",
              " 'input2')"
            ]
          }
        }
      ],
      "source": [
        "large_object = \"Let's pretend that this string is actually a really big object like your dataset\"\n",
        "input1_future = client.scatter(large_object)\n",
        "client.submit(client_test, input1_future, \"input2\").result()"
      ],
      "id": "9ad149a3-2a7c-4a5e-93a9-25647426597b"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-31 06:32:32,584 - distributed.scheduler - ERROR - Removing worker 'tcp://192.168.0.208:38481' caused the cluster to lose scattered data, which can't be recovered: {'str-aed0f69a5b2b8dbc59a28f905628b181'} (stimulus_id='handle-worker-cleanup-1717137152.584407')"
          ]
        }
      ],
      "source": [
        "client.shutdown()"
      ],
      "id": "bd3545f5-a1a6-4be9-b2c3-9f886d2b1f7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If needed we can be more specific about the specific GPU type and QoS we\n",
        "need if we have more complex requirements."
      ],
      "id": "9023349e-1f32-4fc3-aa3b-93ba3f87f2c3"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster = SLURMCluster(\n",
        "    memory=\"128g\", processes=1, cores=16, job_extra_directives=[\"--gres=gpu:3g.20gb:1\", \"--qos=lion\", \"--partition=BigCats\"]\n",
        ")\n",
        "\n",
        "cluster.scale(1)\n",
        "client = Client(cluster)"
      ],
      "id": "b7cb375d-3734-40e7-9d7c-7328c0014388"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "('input1', 'input2')"
            ]
          }
        }
      ],
      "source": [
        "client.submit(client_test, \"input1\", \"input2\", test=True).result()"
      ],
      "id": "4e2814d5-d3ea-4c17-940a-f7fd10cb7c5a"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "db096b8404a4f1f3e1df0cc89f001e138448327417ef835d10f5a76aa612f160"
      }
    }
  }
}